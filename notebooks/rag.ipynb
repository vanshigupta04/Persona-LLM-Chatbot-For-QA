{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !poetry run pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('./Persona-LLM-Chatbot-For-QA')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install python-dotenv\n",
    "# !pip3 install langchain weaviate-client cohere torch transformers \n",
    "# !pip3 install nltk\n",
    "# !pip3 -q install InstructorEmbedding sentence-transformers\n",
    "# !pip3 install ndg-httpsclient\n",
    "# !pip3 install pyopenssl\n",
    "# !pip3 install pyasn1\n",
    "# !unset https_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.environ.get('PINECONE_API_KEY')\n",
    "pinecone_index_name = os.environ.get('PINECONE_INDEX')\n",
    "pinecone_environment = os.environ.get('PINECONE_ENV')\n",
    "cohere_key = os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "# import weaviate\n",
    "\n",
    "# # auth_config = weaviate.AuthApiKey(api_key=\"YOUR-WEAVIATE-API-KEY\")\n",
    "\n",
    "# client = weaviate.Client(\n",
    "#   url='https://friends-fan-persona-lxg9ygzb.weaviate.network',\n",
    "# )\n",
    "\n",
    "# client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/arao/Local/Github/Persona-LLM-Chatbot-For-QA/notebooks/rag.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arao/Local/Github/Persona-LLM-Chatbot-For-QA/notebooks/rag.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m CohereEmbedder\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arao/Local/Github/Persona-LLM-Chatbot-For-QA/notebooks/rag.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m HuggingFaceInstructEmbeddings\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arao/Local/Github/Persona-LLM-Chatbot-For-QA/notebooks/rag.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m instructor_embeddings \u001b[39m=\u001b[39m HuggingFaceInstructEmbeddings(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhkunlp/instructor-xl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arao/Local/Github/Persona-LLM-Chatbot-For-QA/notebooks/rag.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                                                       model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/langchain/embeddings/huggingface.py:149\u001b[0m, in \u001b[0;36mHuggingFaceInstructEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mInstructorEmbedding\u001b[39;00m \u001b[39mimport\u001b[39;00m INSTRUCTOR\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m INSTRUCTOR(\n\u001b[1;32m    150\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name, cache_folder\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_folder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_kwargs\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDependencies for InstructorEmbedding not found.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mflax_model.msgpack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrust_model.ot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtf_model.h5\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_sbert_model(model_path)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:   \u001b[39m#Load with AutoModel\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:474\u001b[0m, in \u001b[0;36mINSTRUCTOR._load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m         module_class \u001b[39m=\u001b[39m import_from_string(module_config[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 474\u001b[0m     module \u001b[39m=\u001b[39m module_class\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_path, module_config[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m    475\u001b[0m     modules[module_config[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m module\n\u001b[1;32m    477\u001b[0m \u001b[39mreturn\u001b[39;00m modules\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:306\u001b[0m, in \u001b[0;36mINSTRUCTOR_Transformer.load\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(sbert_config_path) \u001b[39mas\u001b[39;00m fIn:\n\u001b[1;32m    305\u001b[0m     config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(fIn)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m INSTRUCTOR_Transformer(model_name_or_path\u001b[39m=\u001b[39;49minput_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:240\u001b[0m, in \u001b[0;36mINSTRUCTOR_Transformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args, cache_dir\u001b[39m=\u001b[39mcache_dir)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name_or_path, config, cache_dir, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_args)\n\u001b[1;32m    242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[39mif\u001b[39;00m tokenizer_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_name_or_path, cache_dir\u001b[39m=\u001b[39mcache_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_args)\n\u001b[1;32m    244\u001b[0m \u001b[39m#No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# print('max_seq_length ', max_seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:47\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads the transformer model\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, T5Config):\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39mconfig, cache_dir\u001b[39m=\u001b[39mcache_dir)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:55\u001b[0m, in \u001b[0;36mTransformer._load_t5_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5EncoderModel\n\u001b[1;32m     54\u001b[0m T5EncoderModel\u001b[39m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mdecoder.*\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_model \u001b[39m=\u001b[39m T5EncoderModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path, config\u001b[39m=\u001b[39;49mconfig, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:3480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3472\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3473\u001b[0m     (\n\u001b[1;32m   3474\u001b[0m         model,\n\u001b[1;32m   3475\u001b[0m         missing_keys,\n\u001b[1;32m   3476\u001b[0m         unexpected_keys,\n\u001b[1;32m   3477\u001b[0m         mismatched_keys,\n\u001b[1;32m   3478\u001b[0m         offload_index,\n\u001b[1;32m   3479\u001b[0m         error_msgs,\n\u001b[0;32m-> 3480\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3481\u001b[0m         model,\n\u001b[1;32m   3482\u001b[0m         state_dict,\n\u001b[1;32m   3483\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3484\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3485\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3486\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3487\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3488\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3489\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3490\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3491\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3492\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3493\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3494\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3495\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3496\u001b[0m     )\n\u001b[1;32m   3498\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3499\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:3824\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3815\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3817\u001b[0m         state_dict,\n\u001b[1;32m   3818\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3822\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3823\u001b[0m     )\n\u001b[0;32m-> 3824\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   3825\u001b[0m     offload_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3826\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3827\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3828\u001b[0m \n\u001b[1;32m   3829\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:571\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39;49mstart_prefix)\n\u001b[1;32m    572\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 569 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:569\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:565\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    563\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m         module\u001b[39m.\u001b[39;49m_load_from_state_dict(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    567\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[1;32m   2041\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{\u001b[39;00minput_param\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m.\u001b[39margs\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from rag.vector_database import VectorDatabase\n",
    "# Description: This file contains the VectorDatabase class, which is used to store and search for vectors in Pinecone.\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "# from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "\n",
    "from rag.embeddings import CohereEmbedder\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instructor_embeddings.embed_query(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, embeddings = None, api_key = None, env = None, index_name = None, cohere_api_key = None):\n",
    "        if api_key is None:\n",
    "            api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "            print('PINECONE: Loaded API key from environment variables.')\n",
    "        if env is None:\n",
    "            env = os.environ.get(\"PINECONE_ENV\")\n",
    "            print('PINECONE: Loaded environment from environment variables.')\n",
    "        if index_name is None:\n",
    "            index_name = os.environ.get(\"PINECONE_INDEX\")\n",
    "            print('PINECONE: Loaded index name from environment variables.')\n",
    "        if cohere_api_key is None:\n",
    "            cohere_api_key = os.environ.get(\"COHERE_API_KEY\")\n",
    "            print('COHERE: Loaded API key from environment variables.')\n",
    "\n",
    "        pinecone.init(api_key=api_key, environment=env)\n",
    "        print('PINECONE: initialized')\n",
    "\n",
    "        # if index_name not in pinecone.list_indexes():\n",
    "        #     pinecone.create_index(name=index_name, metric=\"cosine\", dimension=384)\n",
    "        \n",
    "        self.index = pinecone.Index(index_name)\n",
    "        print('PINECONE: Set index to - ', index_name)\n",
    "        \n",
    "        if embeddings == None:\n",
    "            self.embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                model_kwargs={\"device\": \"cuda\"}\n",
    "            )\n",
    "        else:\n",
    "            self.embeddings = embeddings\n",
    "        print('COHERE: loaded embeddings')\n",
    "        \n",
    "        self.vector_search = Pinecone(self.index, self.embeddings.embed_query, \"text\")\n",
    "        \n",
    "    def search(self, query, top_k=128):\n",
    "        return self.vector_search.similarity_search(query, k=top_k)\n",
    "\n",
    "    def upsert(self, data_path: str):\n",
    "        \"\"\"Upserts data into the vector database.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to the data file.\n",
    "        \"\"\"\n",
    "\n",
    "        if '.json' in data_path:\n",
    "            data = pd.read_json(data_path, lines=True, orient='records').to_dict('records')\n",
    "        elif '.csv' in data_path:\n",
    "            data = pd.read_csv(data_path).to_dict('records')\n",
    "        else:\n",
    "            raise Exception('Data format not supported. Please provide a json or csv file.')\n",
    "        \n",
    "        with open('./data/upsert_data.json', 'r') as f:\n",
    "            vectors = json.load(f)\n",
    "\n",
    "        for item in tqdm(data, desc=\"Processing data\", unit=\"row\", ncols=80):\n",
    "            conversation_id = item.get(\"conversation_id\")\n",
    "            speaker = item.get(\"speaker\")\n",
    "            season = item.get(\"season\")\n",
    "            episode = item.get(\"episode\")\n",
    "            scene = item.get(\"scene\")\n",
    "            text = item.get(\"text\")\n",
    "\n",
    "            if conversation_id and speaker and text and season and episode and scene:\n",
    "                metadata = {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"season\": season,\n",
    "                    \"episode\": episode,\n",
    "                    \"scene\": scene\n",
    "                }\n",
    "\n",
    "                vector = self.embeddings.embed_query(text)\n",
    "                \n",
    "                record_metadata = {\n",
    "                    \"text\": text, \n",
    "                    \"source\": str(metadata)\n",
    "                }\n",
    "\n",
    "                # Perform a single upsert operation with all the data\n",
    "                self.index.upsert(vectors=[{'id': conversation_id, 'values': vector, 'metadata': record_metadata}])\n",
    "\n",
    "        print(f\"Upserted {len(upsert_data)} vectors into the database.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VectorDatabase(embeddings = instructor_embeddings, api_key=pinecone_api_key, index_name=pinecone_index_name, env=pinecone_environment, cohere_api_key=cohere_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path =  '../notebooks/data/friends_dataset.jsonl'\n",
    "data_path =  '../data/friends_dataset.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/upsert_data.json', 'r') as f:\n",
    "#     vectors = json.load(f)\n",
    "\n",
    "# data = pd.read_json(data_path, lines=True, orient='records').to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pinecone\n",
    "# pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
    "# index = pinecone.Index(pinecone_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in tqdm(range(18078+11199+153+24801+4854,len(data)), desc=\"Processing data\", unit=\"row\", ncols=80):\n",
    "#     conversation_id = data[item].get(\"conversation_id\")\n",
    "#     speaker = data[item].get(\"speaker\")\n",
    "#     season = data[item].get(\"season\")\n",
    "#     episode = data[item].get(\"episode\")\n",
    "#     scene = data[item].get(\"scene\")\n",
    "#     text = data[item].get(\"text\")\n",
    "\n",
    "#     if conversation_id and speaker and text and season and episode and scene:\n",
    "#         metadata = {\n",
    "#             \"speaker\": speaker,\n",
    "#             \"season\": season,\n",
    "#             \"episode\": episode,\n",
    "#             \"scene\": scene\n",
    "#         }\n",
    "\n",
    "#         # print(metadata)\n",
    "\n",
    "#         # vectors = self.embeddings.embed_query(text)\n",
    "#         vector = vectors[num]['values']\n",
    "#         num+=1\n",
    "\n",
    "#         record_metadata = {\n",
    "#             \"text\": text, \n",
    "#             \"source\": str(metadata)\n",
    "#         }\n",
    "\n",
    "#         # Perform a single upsert operation with all the data\n",
    "#         index.upsert(vectors=[{'id': conversation_id, 'values': vector, 'metadata': record_metadata}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|███████████████████| 67373/67373 [27:34<00:00, 40.73row/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 61310 vectors into the database.\n"
     ]
    }
   ],
   "source": [
    "db.upsert(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config.ini']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arao/Library/Caches/pypoetry/virtualenvs/persona_llm_for_qa_chatbot-OZ20-Od0-py3.10/lib/python3.10/site-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:  input_variables=['context'] template='[INST]<<SYS>>             \"You are an expert at answering questions about the Friends TV Show. I want you to answer the following question to the point and keep the answer short.\"             <<SYS>>             {context}[/INST]'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from utils.llm import CustomLLM\n",
    "\n",
    "\n",
    "\n",
    "# completion llm\n",
    "llm = CustomLLM()\n",
    "\n",
    "embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# vectorstore\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n",
    "index = pinecone.Index(pinecone_index_name)\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings.embed_query, \"text\"\n",
    ")\n",
    "\n",
    "# prompt\n",
    "# prompt = PromptTemplate(\n",
    "#             template=llm.format_prompt(\n",
    "#                 user_query=\"{context}\",\n",
    "#                 system_instruction=config.get('llm.prompt', 'system_instruction'),\n",
    "#             ),\n",
    "#             input_variables=[\"context\"],\n",
    "#         )\n",
    "\n",
    "\n",
    "# retrieval qa\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "   # chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Oh, great! *nervous laugh* Uh, yeah, Ross and Rachel's relationship is... complicated. *glances around nervously* But hey, at least they have each other... *forced smile*\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\n",
    "    llm.format_prompt(\n",
    "        user_query=\"What episode did ross marry rachel?\",\n",
    "        system_instruction=config.get('llm.prompt', 'system_instruction'),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
