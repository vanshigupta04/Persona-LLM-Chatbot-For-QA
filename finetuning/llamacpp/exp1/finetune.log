
Ubuntu 22.04.3 LTS5.15.0-78-generic
----------------------------------------------------------------------------
Machine Name:  	rlab1.cs          IP No:       128.6.13.7 2620:0:d60:ac0d::7
Thu Dec  7 02:58:16 PM EST 2023	  Uptime:        	      118 days 01:42
----------------------------------------------------------------------------
Processes:     	1111              Local/SSH/X2Go/XRDP/VSCODE:	0/3/0/0/0           
HostProxy:     	0                 TMUX/SCREEN/JUPYTER:	6/1/0
Connections:   	14                Load/Total Users:	2/10
Free Memory:   	798Gi of 1.5Ti    Free Swap:     	511Gi of 511Gi
----------------------------------------------------------------------------
CPU Info:      	AMD EPYC 7413 24-Core Processor - 96 cores 
System CPU:    	0.12%             User CPU:      	0.13%
CPU Idle:      	99.76%            IO Wait:       	0.00%
----------------------------------------------------------------------------
Login as:      	za224             No. of Sessions:	2
Avail.UserDisk:	                  Avail.Freespace:	5861.67 GB
CUDA Driver:   	11.8              CUDA Cores:    	14336
----------------------------------------------------------------------------

ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 4 CUDA devices:
  Device 0: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1
  Device 1: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1
  Device 2: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1
  Device 3: NVIDIA GeForce GTX 1080 Ti, compute capability 6.1
llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /freespace/local/za224/llama.cpp/models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)
llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]
llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]
llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]
llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]
llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]
llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V2
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = mostly Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce GTX 1080 Ti) as main device
llm_load_tensors: mem required  = 3891.35 MiB
llm_load_tensors: offloading 0 repeating layers to GPU
llm_load_tensors: offloaded 0/35 layers to GPU
llm_load_tensors: VRAM used: 0.00 MiB
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: kv self size  =  256.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.57 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 70.50 MiB (model: 0.00 MiB, context: 70.50 MiB)
main: seed: 1
main: model base = '/freespace/local/za224/llama.cpp/models/llama-2-7b-chat.Q4_K_M.gguf'
main: init model
print_params: n_vocab:   32000
print_params: n_ctx:     256
print_params: n_embd:    4096
print_params: n_ff:      11008
print_params: n_head:    32
print_params: n_head_kv: 32
print_params: n_layer:   32
print_params: norm_rms_eps          : 0.000001
print_params: rope_freq_base        : 10000.000000
print_params: rope_freq_scale       : 1.000000
print_lora_params: n_rank_attention_norm : 1
print_lora_params: n_rank_wq             : 4
print_lora_params: n_rank_wk             : 4
print_lora_params: n_rank_wv             : 4
print_lora_params: n_rank_wo             : 4
print_lora_params: n_rank_ffn_norm       : 1
print_lora_params: n_rank_w1             : 4
print_lora_params: n_rank_w2             : 4
print_lora_params: n_rank_w3             : 4
print_lora_params: n_rank_tok_embeddings : 4
print_lora_params: n_rank_norm           : 1
print_lora_params: n_rank_output         : 4
main: total train_iterations 0
main: seen train_samples     0
main: seen train_tokens      0
main: completed train_epochs 0
main: lora_size = 84845152 bytes (80.9 MB)
main: opt_size  = 126592960 bytes (120.7 MB)
main: opt iter 0
main: input_size = 32769056 bytes (31.3 MB)
main: compute_size = 4353442912 bytes (4151.8 MB)
main: evaluation order = RIGHT_TO_LEFT
main: tokenize training data
tokenize_file: warning: found 1 samples (max length 292) that exceed context length of 256. samples will be cut off.
tokenize_file: warning: found 61336 samples (min length 6) that are shorter than context length of 256.
tokenize_file: total number of samples: 61337
main: number of training tokens: 1441492
main: number of unique tokens: 10820
main: train data seems to have changed. restarting shuffled epoch.
main: begin training
main: work_size = 1792920 bytes (1.7 MB)
train_opt_callback: iter=     0 sample=1/61337 sched=0.000000 loss=0.000000 |->
train_opt_callback: iter=     1 sample=2/61337 sched=0.010000 loss=7.901807 dt=00:01:42 eta=07:14:27 |->
train_opt_callback: iter=     2 sample=3/61337 sched=0.020000 loss=8.396348 dt=00:01:41 eta=07:09:18 |>
train_opt_callback: iter=     3 sample=4/61337 sched=0.030000 loss=7.674074 dt=00:01:41 eta=07:06:20 |--->
train_opt_callback: iter=     4 sample=5/61337 sched=0.040000 loss=7.374213 dt=00:01:42 eta=07:09:08 |------>
train_opt_callback: iter=     5 sample=6/61337 sched=0.050000 loss=7.284311 dt=00:01:42 eta=07:09:08 |------->
train_opt_callback: iter=     6 sample=7/61337 sched=0.060000 loss=7.085641 dt=00:01:40 eta=06:59:17 |--------->
train_opt_callback: iter=     7 sample=8/61337 sched=0.070000 loss=6.282666 dt=00:01:40 eta=06:57:10 |----------------->
train_opt_callback: iter=     8 sample=9/61337 sched=0.080000 loss=5.846704 dt=00:01:40 eta=06:56:28 |---------------------->
train_opt_callback: iter=     9 sample=10/61337 sched=0.090000 loss=4.607859 dt=00:01:41 eta=06:59:42 |---------------------------------->
train_opt_callback: iter=    10 sample=11/61337 sched=0.100000 loss=3.560176 dt=00:01:41 eta=06:54:55 |-------------------------------------------->
train_opt_callback: iter=    11 sample=12/61337 sched=0.110000 loss=2.417872 dt=00:01:40 eta=06:51:28 |-------------------------------------------------------->
train_opt_callback: iter=    12 sample=13/61337 sched=0.120000 loss=1.700649 dt=00:01:40 eta=06:49:06 |--------------------------------------------------------------->
train_opt_callback: iter=    13 sample=14/61337 sched=0.130000 loss=1.524246 dt=00:01:41 eta=06:52:13 |----------------------------------------------------------------->
train_opt_callback: iter=    14 sample=15/61337 sched=0.140000 loss=1.562454 dt=00:01:40 eta=06:45:46 |---------------------------------------------------------------->
train_opt_callback: iter=    15 sample=16/61337 sched=0.150000 loss=0.740593 dt=00:01:40 eta=06:44:35 |------------------------------------------------------------------------->
train_opt_callback: iter=    16 sample=17/61337 sched=0.160000 loss=1.062463 dt=00:01:40 eta=06:43:57 |--------------------------------------------------------------------->
train_opt_callback: iter=    17 sample=18/61337 sched=0.170000 loss=0.373131 dt=00:01:42 eta=06:46:30 |---------------------------------------------------------------------------->
train_opt_callback: iter=    18 sample=19/61337 sched=0.180000 loss=0.644715 dt=00:01:41 eta=06:41:30 |-------------------------------------------------------------------------->
train_opt_callback: iter=    19 sample=20/61337 sched=0.190000 loss=0.679512 dt=00:01:40 eta=06:38:02 |------------------------------------------------------------------------->
train_opt_callback: iter=    20 sample=21/61337 sched=0.200000 loss=0.370934 dt=00:01:41 eta=06:40:03 |---------------------------------------------------------------------------->
train_opt_callback: iter=    21 sample=22/61337 sched=0.210000 loss=0.263296 dt=00:01:42 eta=06:41:03 |----------------------------------------------------------------------------->
train_opt_callback: iter=    22 sample=23/61337 sched=0.220000 loss=0.411652 dt=00:01:42 eta=06:39:11 |---------------------------------------------------------------------------->
train_opt_callback: iter=    23 sample=24/61337 sched=0.230000 loss=0.226909 dt=00:01:41 eta=06:34:58 |------------------------------------------------------------------------------>
train_opt_callback: iter=    24 sample=25/61337 sched=0.240000 loss=0.217757 dt=00:01:40 eta=06:30:16 |------------------------------------------------------------------------------>
train_opt_callback: iter=    25 sample=26/61337 sched=0.250000 loss=0.266732 dt=00:01:41 eta=06:32:04 |----------------------------------------------------------------------------->
train_opt_callback: iter=    26 sample=27/61337 sched=0.260000 loss=0.261217 dt=00:01:41 eta=06:27:38 |----------------------------------------------------------------------------->
train_opt_callback: iter=    27 sample=28/61337 sched=0.270000 loss=0.524819 dt=00:01:41 eta=06:26:37 |--------------------------------------------------------------------------->
train_opt_callback: iter=    28 sample=29/61337 sched=0.280000 loss=0.438537 dt=00:01:41 eta=06:25:19 |---------------------------------------------------------------------------->
train_opt_callback: iter=    29 sample=30/61337 sched=0.290000 loss=0.423370 dt=00:01:41 eta=06:23:24 |---------------------------------------------------------------------------->
train_opt_callback: iter=    30 sample=31/61337 sched=0.300000 loss=0.174289 dt=00:01:41 eta=06:23:14 |------------------------------------------------------------------------------>
train_opt_callback: iter=    31 sample=32/61337 sched=0.310000 loss=0.168342 dt=00:01:42 eta=06:23:04 |------------------------------------------------------------------------------>
train_opt_callback: iter=    32 sample=33/61337 sched=0.320000 loss=0.257468 dt=00:01:41 eta=06:17:16 |----------------------------------------------------------------------------->
train_opt_callback: iter=    33 sample=34/61337 sched=0.330000 loss=1.037117 dt=00:01:41 eta=06:16:35 |---------------------------------------------------------------------->
train_opt_callback: iter=    34 sample=35/61337 sched=0.340000 loss=0.161562 dt=00:01:40 eta=06:13:00 |------------------------------------------------------------------------------>
train_opt_callback: iter=    35 sample=36/61337 sched=0.350000 loss=0.349061 dt=00:01:42 eta=06:17:03 |----------------------------------------------------------------------------->
train_opt_callback: iter=    36 sample=37/61337 sched=0.360000 loss=0.228936 dt=00:01:42 eta=06:14:25 |------------------------------------------------------------------------------>
train_opt_callback: iter=    37 sample=38/61337 sched=0.370000 loss=0.183329 dt=00:01:40 eta=06:08:05 |------------------------------------------------------------------------------>
train_opt_callback: iter=    38 sample=39/61337 sched=0.380000 loss=0.145797 dt=00:01:40 eta=06:06:07 |------------------------------------------------------------------------------->
train_opt_callback: iter=    39 sample=40/61337 sched=0.390000 loss=0.456879 dt=00:01:41 eta=06:08:04 |--------------------------------------------------------------------------->
train_opt_callback: iter=    40 sample=41/61337 sched=0.400000 loss=0.120358 dt=00:01:41 eta=06:04:03 |------------------------------------------------------------------------------->
train_opt_callback: iter=    41 sample=42/61337 sched=0.410000 loss=0.958121 dt=00:01:41 eta=06:05:02 |---------------------------------------------------------------------->
train_opt_callback: iter=    42 sample=43/61337 sched=0.420000 loss=0.203396 dt=00:01:40 eta=05:59:12 |------------------------------------------------------------------------------>
train_opt_callback: iter=    43 sample=44/61337 sched=0.430000 loss=0.250856 dt=00:01:41 eta=05:59:41 |------------------------------------------------------------------------------>
train_opt_callback: iter=    44 sample=45/61337 sched=0.440000 loss=0.267565 dt=00:01:40 eta=05:55:53 |----------------------------------------------------------------------------->
train_opt_callback: iter=    45 sample=46/61337 sched=0.450000 loss=0.282623 dt=00:01:40 eta=05:54:05 |----------------------------------------------------------------------------->
train_opt_callback: iter=    46 sample=47/61337 sched=0.460000 loss=0.118368 dt=00:01:40 eta=05:50:24 |------------------------------------------------------------------------------->
train_opt_callback: iter=    47 sample=48/61337 sched=0.470000 loss=0.242005 dt=00:01:41 eta=05:53:17 |------------------------------------------------------------------------------>
train_opt_callback: iter=    48 sample=49/61337 sched=0.480000 loss=0.158809 dt=00:01:42 eta=05:54:13 |------------------------------------------------------------------------------>
train_opt_callback: iter=    49 sample=50/61337 sched=0.490000 loss=0.080453 dt=00:01:41 eta=05:51:05 |------------------------------------------------------------------------------->
train_opt_callback: iter=    50 sample=51/61337 sched=0.500000 loss=0.159367 dt=00:01:40 eta=05:44:52 |------------------------------------------------------------------------------>
train_opt_callback: iter=    51 sample=52/61337 sched=0.510000 loss=0.116152 dt=00:01:41 eta=05:46:22 |------------------------------------------------------------------------------->
train_opt_callback: iter=    52 sample=53/61337 sched=0.520000 loss=0.070871 dt=00:01:41 eta=05:44:57 |------------------------------------------------------------------------------->
train_opt_callback: iter=    53 sample=54/61337 sched=0.530000 loss=0.141863 dt=00:01:40 eta=05:38:29 |------------------------------------------------------------------------------->
train_opt_callback: iter=    54 sample=55/61337 sched=0.540000 loss=0.105641 dt=00:01:40 eta=05:38:24 |------------------------------------------------------------------------------->
train_opt_callback: iter=    55 sample=56/61337 sched=0.550000 loss=0.225376 dt=00:01:41 eta=05:38:25 |------------------------------------------------------------------------------>
train_opt_callback: iter=    56 sample=57/61337 sched=0.560000 loss=0.125130 dt=00:01:41 eta=05:39:43 |------------------------------------------------------------------------------->
train_opt_callback: iter=    57 sample=58/61337 sched=0.570000 loss=0.110359 dt=00:01:42 eta=05:39:12 |------------------------------------------------------------------------------->
train_opt_callback: iter=    58 sample=59/61337 sched=0.580000 loss=0.266634 dt=00:01:41 eta=05:33:30 |----------------------------------------------------------------------------->
train_opt_callback: iter=    59 sample=60/61337 sched=0.590000 loss=0.085531 dt=00:01:40 eta=05:31:21 |------------------------------------------------------------------------------->
train_opt_callback: iter=    60 sample=61/61337 sched=0.600000 loss=0.043286 dt=00:01:41 eta=05:30:48 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    61 sample=62/61337 sched=0.610000 loss=0.286257 dt=00:01:41 eta=05:30:58 |----------------------------------------------------------------------------->
train_opt_callback: iter=    62 sample=63/61337 sched=0.620000 loss=0.077501 dt=00:01:41 eta=05:27:11 |------------------------------------------------------------------------------->
train_opt_callback: iter=    63 sample=64/61337 sched=0.630000 loss=0.140226 dt=00:01:41 eta=05:25:25 |------------------------------------------------------------------------------->
train_opt_callback: iter=    64 sample=65/61337 sched=0.640000 loss=0.278407 dt=00:01:40 eta=05:21:25 |----------------------------------------------------------------------------->
train_opt_callback: iter=    65 sample=66/61337 sched=0.650000 loss=0.212665 dt=00:01:41 eta=05:23:33 |------------------------------------------------------------------------------>
train_opt_callback: iter=    66 sample=67/61337 sched=0.660000 loss=0.129530 dt=00:01:40 eta=05:19:38 |------------------------------------------------------------------------------->
train_opt_callback: iter=    67 sample=68/61337 sched=0.670000 loss=0.037520 dt=00:01:40 eta=05:17:33 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    68 sample=69/61337 sched=0.680000 loss=0.099729 dt=00:01:41 eta=05:17:20 |------------------------------------------------------------------------------->
train_opt_callback: iter=    69 sample=70/61337 sched=0.690000 loss=0.122045 dt=00:01:41 eta=05:16:21 |------------------------------------------------------------------------------->
train_opt_callback: iter=    70 sample=71/61337 sched=0.700000 loss=0.031105 dt=00:01:40 eta=05:12:45 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    71 sample=72/61337 sched=0.710000 loss=0.056185 dt=00:01:41 eta=05:11:32 |------------------------------------------------------------------------------->
train_opt_callback: iter=    72 sample=73/61337 sched=0.720000 loss=0.037004 dt=00:01:40 eta=05:09:05 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    73 sample=74/61337 sched=0.730000 loss=0.744566 dt=00:01:41 eta=05:10:51 |------------------------------------------------------------------------->
train_opt_callback: iter=    74 sample=75/61337 sched=0.740000 loss=0.626689 dt=00:01:42 eta=05:09:54 |-------------------------------------------------------------------------->
train_opt_callback: iter=    75 sample=76/61337 sched=0.750000 loss=0.130895 dt=00:01:41 eta=05:05:04 |------------------------------------------------------------------------------->
train_opt_callback: iter=    76 sample=77/61337 sched=0.760000 loss=0.169804 dt=00:01:42 eta=05:06:19 |------------------------------------------------------------------------------>
train_opt_callback: iter=    77 sample=78/61337 sched=0.770000 loss=0.136545 dt=00:01:42 eta=05:04:23 |------------------------------------------------------------------------------->
train_opt_callback: iter=    78 sample=79/61337 sched=0.780000 loss=0.234421 dt=00:01:40 eta=04:59:33 |------------------------------------------------------------------------------>
train_opt_callback: iter=    79 sample=80/61337 sched=0.790000 loss=0.179926 dt=00:01:41 eta=04:58:04 |------------------------------------------------------------------------------>
train_opt_callback: iter=    80 sample=81/61337 sched=0.800000 loss=0.136379 dt=00:01:40 eta=04:56:02 |------------------------------------------------------------------------------->
train_opt_callback: iter=    81 sample=82/61337 sched=0.810000 loss=0.057447 dt=00:01:41 eta=04:55:09 |------------------------------------------------------------------------------->
train_opt_callback: iter=    82 sample=83/61337 sched=0.820000 loss=0.130756 dt=00:01:41 eta=04:54:38 |------------------------------------------------------------------------------->
train_opt_callback: iter=    83 sample=84/61337 sched=0.830000 loss=0.235767 dt=00:01:40 eta=04:50:56 |------------------------------------------------------------------------------>
train_opt_callback: iter=    84 sample=85/61337 sched=0.840000 loss=0.166863 dt=00:01:40 eta=04:47:20 |------------------------------------------------------------------------------>
train_opt_callback: iter=    85 sample=86/61337 sched=0.850000 loss=0.243397 dt=00:01:40 eta=04:46:14 |------------------------------------------------------------------------------>
train_opt_callback: iter=    86 sample=87/61337 sched=0.860000 loss=0.051724 dt=00:01:41 eta=04:47:32 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    87 sample=88/61337 sched=0.870000 loss=0.098426 dt=00:01:40 eta=04:42:56 |------------------------------------------------------------------------------->
train_opt_callback: iter=    88 sample=89/61337 sched=0.880000 loss=0.087912 dt=00:01:41 eta=04:45:01 |------------------------------------------------------------------------------->
train_opt_callback: iter=    89 sample=90/61337 sched=0.890000 loss=0.519294 dt=00:01:41 eta=04:41:13 |--------------------------------------------------------------------------->
train_opt_callback: iter=    90 sample=91/61337 sched=0.900000 loss=0.107562 dt=00:01:41 eta=04:40:28 |------------------------------------------------------------------------------->
train_opt_callback: iter=    91 sample=92/61337 sched=0.910000 loss=0.210948 dt=00:01:40 eta=04:36:23 |------------------------------------------------------------------------------>
train_opt_callback: iter=    92 sample=93/61337 sched=0.920000 loss=0.154043 dt=00:01:40 eta=04:35:41 |------------------------------------------------------------------------------>
train_opt_callback: iter=    93 sample=94/61337 sched=0.930000 loss=0.208101 dt=00:01:40 eta=04:33:30 |------------------------------------------------------------------------------>
train_opt_callback: iter=    94 sample=95/61337 sched=0.940000 loss=0.041662 dt=00:01:41 eta=04:33:54 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    95 sample=96/61337 sched=0.950000 loss=0.046505 dt=00:01:41 eta=04:33:27 |-------------------------------------------------------------------------------->
train_opt_callback: iter=    96 sample=97/61337 sched=0.960000 loss=0.200879 dt=00:01:41 eta=04:29:24 |------------------------------------------------------------------------------>
train_opt_callback: iter=    97 sample=98/61337 sched=0.970000 loss=0.189824 dt=00:01:41 eta=04:27:58 |------------------------------------------------------------------------------>
train_opt_callback: iter=    98 sample=99/61337 sched=0.980000 loss=0.170753 dt=00:01:42 eta=04:29:18 |------------------------------------------------------------------------------>
train_opt_callback: iter=    99 sample=100/61337 sched=0.990000 loss=0.305511 dt=00:01:42 eta=04:27:24 |----------------------------------------------------------------------------->
train_opt_callback: iter=   100 sample=101/61337 sched=0.977975 loss=0.615115 dt=00:01:42 eta=04:27:28 |-------------------------------------------------------------------------->
train_opt_callback: iter=   101 sample=102/61337 sched=0.977536 loss=0.133748 dt=00:01:42 eta=04:23:37 |------------------------------------------------------------------------------->
train_opt_callback: iter=   102 sample=103/61337 sched=0.977093 loss=0.142589 dt=00:01:41 eta=04:19:43 |------------------------------------------------------------------------------->
train_opt_callback: iter=   103 sample=104/61337 sched=0.976646 loss=0.268735 dt=00:01:40 eta=04:16:49 |----------------------------------------------------------------------------->
train_opt_callback: iter=   104 sample=105/61337 sched=0.976194 loss=0.192208 dt=00:01:42 eta=04:18:50 |------------------------------------------------------------------------------>
train_opt_callback: iter=   105 sample=106/61337 sched=0.975738 loss=0.173360 dt=00:01:42 eta=04:16:59 |------------------------------------------------------------------------------>
train_opt_callback: iter=   106 sample=107/61337 sched=0.975278 loss=0.375550 dt=00:01:41 eta=04:14:44 |---------------------------------------------------------------------------->
train_opt_callback: iter=   107 sample=108/61337 sched=0.974814 loss=0.615037 dt=00:01:41 eta=04:11:43 |-------------------------------------------------------------------------->
train_opt_callback: iter=   108 sample=109/61337 sched=0.974346 loss=0.090633 dt=00:01:41 eta=04:10:46 |------------------------------------------------------------------------------->
train_opt_callback: iter=   109 sample=110/61337 sched=0.973873 loss=0.947918 dt=00:01:41 eta=04:07:28 |----------------------------------------------------------------------->
train_opt_callback: iter=   110 sample=111/61337 sched=0.973396 loss=0.078215 dt=00:01:40 eta=04:03:34 |------------------------------------------------------------------------------->
train_opt_callback: iter=   111 sample=112/61337 sched=0.972915 loss=0.235154 dt=00:01:40 eta=04:04:03 |------------------------------------------------------------------------------>
train_opt_callback: iter=   112 sample=113/61337 sched=0.972430 loss=0.754060 dt=00:01:41 eta=04:04:04 |------------------------------------------------------------------------>
train_opt_callback: iter=   113 sample=114/61337 sched=0.971941 loss=0.058666 dt=00:01:42 eta=04:03:42 |------------------------------------------------------------------------------->
train_opt_callback: iter=   114 sample=115/61337 sched=0.971447 loss=0.668549 dt=00:01:41 eta=04:00:57 |------------------------------------------------------------------------->
train_opt_callback: iter=   115 sample=116/61337 sched=0.970950 loss=0.042769 dt=00:01:41 eta=03:58:35 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   116 sample=117/61337 sched=0.970448 loss=0.146902 dt=00:01:40 eta=03:55:10 |------------------------------------------------------------------------------->
train_opt_callback: iter=   117 sample=118/61337 sched=0.969942 loss=0.287127 dt=00:01:40 eta=03:53:07 |----------------------------------------------------------------------------->
train_opt_callback: iter=   118 sample=119/61337 sched=0.969432 loss=0.259972 dt=00:01:41 eta=03:52:25 |----------------------------------------------------------------------------->
train_opt_callback: iter=   119 sample=120/61337 sched=0.968918 loss=0.535989 dt=00:01:40 eta=03:49:03 |--------------------------------------------------------------------------->
train_opt_callback: iter=   120 sample=121/61337 sched=0.968399 loss=0.278460 dt=00:01:40 eta=03:46:58 |----------------------------------------------------------------------------->
train_opt_callback: iter=   121 sample=122/61337 sched=0.967877 loss=0.261930 dt=00:01:40 eta=03:46:41 |----------------------------------------------------------------------------->
train_opt_callback: iter=   122 sample=123/61337 sched=0.967350 loss=0.217016 dt=00:01:41 eta=03:45:59 |------------------------------------------------------------------------------>
train_opt_callback: iter=   123 sample=124/61337 sched=0.966820 loss=0.097561 dt=00:01:41 eta=03:44:40 |------------------------------------------------------------------------------->
train_opt_callback: iter=   124 sample=125/61337 sched=0.966285 loss=0.102535 dt=00:01:41 eta=03:43:08 |------------------------------------------------------------------------------->
train_opt_callback: iter=   125 sample=126/61337 sched=0.965746 loss=0.054634 dt=00:01:40 eta=03:39:54 |------------------------------------------------------------------------------->
train_opt_callback: iter=   126 sample=127/61337 sched=0.965203 loss=0.045125 dt=00:01:41 eta=03:39:33 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   127 sample=128/61337 sched=0.964656 loss=1.000883 dt=00:01:41 eta=03:38:03 |---------------------------------------------------------------------->
train_opt_callback: iter=   128 sample=129/61337 sched=0.964104 loss=0.618202 dt=00:01:40 eta=03:35:24 |-------------------------------------------------------------------------->
train_opt_callback: iter=   129 sample=130/61337 sched=0.963549 loss=0.397903 dt=00:01:40 eta=03:32:31 |---------------------------------------------------------------------------->
train_opt_callback: iter=   130 sample=131/61337 sched=0.962990 loss=0.046787 dt=00:01:41 eta=03:33:22 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   131 sample=132/61337 sched=0.962426 loss=0.162678 dt=00:01:42 eta=03:33:21 |------------------------------------------------------------------------------>
train_opt_callback: iter=   132 sample=133/61337 sched=0.961859 loss=0.057843 dt=00:01:42 eta=03:30:55 |------------------------------------------------------------------------------->
train_opt_callback: iter=   133 sample=134/61337 sched=0.961287 loss=0.076050 dt=00:01:41 eta=03:27:14 |------------------------------------------------------------------------------->
train_opt_callback: iter=   134 sample=135/61337 sched=0.960711 loss=0.204617 dt=00:01:40 eta=03:24:46 |------------------------------------------------------------------------------>
train_opt_callback: iter=   135 sample=136/61337 sched=0.960131 loss=0.226020 dt=00:01:42 eta=03:26:20 |------------------------------------------------------------------------------>
train_opt_callback: iter=   136 sample=137/61337 sched=0.959548 loss=0.269708 dt=00:01:40 eta=03:21:38 |----------------------------------------------------------------------------->
train_opt_callback: iter=   137 sample=138/61337 sched=0.958960 loss=0.066634 dt=00:01:40 eta=03:19:45 |------------------------------------------------------------------------------->
train_opt_callback: iter=   138 sample=139/61337 sched=0.958368 loss=0.106261 dt=00:01:40 eta=03:17:09 |------------------------------------------------------------------------------->
train_opt_callback: iter=   139 sample=140/61337 sched=0.957772 loss=0.187640 dt=00:01:40 eta=03:16:05 |------------------------------------------------------------------------------>
train_opt_callback: iter=   140 sample=141/61337 sched=0.957172 loss=0.305415 dt=00:01:40 eta=03:14:19 |----------------------------------------------------------------------------->
train_opt_callback: iter=   141 sample=142/61337 sched=0.956568 loss=0.198423 dt=00:01:41 eta=03:13:38 |------------------------------------------------------------------------------>
train_opt_callback: iter=   142 sample=143/61337 sched=0.955960 loss=0.128451 dt=00:01:41 eta=03:12:11 |------------------------------------------------------------------------------->
train_opt_callback: iter=   143 sample=144/61337 sched=0.955348 loss=0.051899 dt=00:01:41 eta=03:11:22 |------------------------------------------------------------------------------->
train_opt_callback: iter=   144 sample=145/61337 sched=0.954732 loss=0.184672 dt=00:01:40 eta=03:07:18 |------------------------------------------------------------------------------>
train_opt_callback: iter=   145 sample=146/61337 sched=0.954112 loss=0.142845 dt=00:01:40 eta=03:05:48 |------------------------------------------------------------------------------->
train_opt_callback: iter=   146 sample=147/61337 sched=0.953488 loss=0.145473 dt=00:01:41 eta=03:05:20 |------------------------------------------------------------------------------->
train_opt_callback: iter=   147 sample=148/61337 sched=0.952861 loss=0.322897 dt=00:01:42 eta=03:06:43 |----------------------------------------------------------------------------->
train_opt_callback: iter=   148 sample=149/61337 sched=0.952229 loss=0.106673 dt=00:01:42 eta=03:05:04 |------------------------------------------------------------------------------->
train_opt_callback: iter=   149 sample=150/61337 sched=0.951593 loss=0.273564 dt=00:01:42 eta=03:02:11 |----------------------------------------------------------------------------->
train_opt_callback: iter=   150 sample=151/61337 sched=0.950953 loss=0.029542 dt=00:01:41 eta=02:58:28 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   151 sample=152/61337 sched=0.950309 loss=0.160449 dt=00:01:41 eta=02:57:07 |------------------------------------------------------------------------------>
train_opt_callback: iter=   152 sample=153/61337 sched=0.949661 loss=0.156917 dt=00:01:42 eta=02:57:01 |------------------------------------------------------------------------------>
train_opt_callback: iter=   153 sample=154/61337 sched=0.949010 loss=0.423304 dt=00:01:40 eta=02:53:11 |---------------------------------------------------------------------------->
train_opt_callback: iter=   154 sample=155/61337 sched=0.948354 loss=0.116439 dt=00:01:39 eta=02:49:49 |------------------------------------------------------------------------------->
train_opt_callback: iter=   155 sample=156/61337 sched=0.947695 loss=0.435927 dt=00:01:41 eta=02:50:53 |---------------------------------------------------------------------------->
train_opt_callback: iter=   156 sample=157/61337 sched=0.947031 loss=0.177910 dt=00:01:40 eta=02:47:45 |------------------------------------------------------------------------------>
train_opt_callback: iter=   157 sample=158/61337 sched=0.946364 loss=0.078357 dt=00:01:40 eta=02:45:31 |------------------------------------------------------------------------------->
train_opt_callback: iter=   158 sample=159/61337 sched=0.945692 loss=0.073222 dt=00:01:42 eta=02:46:55 |------------------------------------------------------------------------------->
train_opt_callback: iter=   159 sample=160/61337 sched=0.945017 loss=0.256150 dt=00:01:40 eta=02:43:15 |----------------------------------------------------------------------------->
train_opt_callback: iter=   160 sample=161/61337 sched=0.944338 loss=0.231598 dt=00:01:41 eta=02:41:40 |------------------------------------------------------------------------------>
train_opt_callback: iter=   161 sample=162/61337 sched=0.943655 loss=0.136869 dt=00:01:41 eta=02:40:12 |------------------------------------------------------------------------------->
train_opt_callback: iter=   162 sample=163/61337 sched=0.942968 loss=0.216052 dt=00:01:42 eta=02:39:58 |------------------------------------------------------------------------------>
train_opt_callback: iter=   163 sample=164/61337 sched=0.942277 loss=0.197564 dt=00:01:40 eta=02:36:25 |------------------------------------------------------------------------------>
train_opt_callback: iter=   164 sample=165/61337 sched=0.941583 loss=0.142070 dt=00:01:40 eta=02:34:25 |------------------------------------------------------------------------------->
train_opt_callback: iter=   165 sample=166/61337 sched=0.940884 loss=0.318971 dt=00:01:41 eta=02:33:24 |----------------------------------------------------------------------------->
train_opt_callback: iter=   166 sample=167/61337 sched=0.940182 loss=0.143997 dt=00:01:40 eta=02:31:07 |------------------------------------------------------------------------------->
train_opt_callback: iter=   167 sample=168/61337 sched=0.939476 loss=0.073310 dt=00:01:41 eta=02:30:06 |------------------------------------------------------------------------------->
train_opt_callback: iter=   168 sample=169/61337 sched=0.938765 loss=0.034057 dt=00:01:41 eta=02:28:40 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   169 sample=170/61337 sched=0.938052 loss=0.277597 dt=00:01:41 eta=02:27:43 |----------------------------------------------------------------------------->
train_opt_callback: iter=   170 sample=171/61337 sched=0.937334 loss=0.222781 dt=00:01:40 eta=02:24:20 |------------------------------------------------------------------------------>
train_opt_callback: iter=   171 sample=172/61337 sched=0.936612 loss=0.130556 dt=00:01:40 eta=02:22:01 |------------------------------------------------------------------------------->
train_opt_callback: iter=   172 sample=173/61337 sched=0.935887 loss=0.389721 dt=00:01:40 eta=02:21:08 |---------------------------------------------------------------------------->
train_opt_callback: iter=   173 sample=174/61337 sched=0.935158 loss=0.247289 dt=00:01:41 eta=02:20:23 |------------------------------------------------------------------------------>
train_opt_callback: iter=   174 sample=175/61337 sched=0.934425 loss=0.064801 dt=00:01:40 eta=02:17:56 |------------------------------------------------------------------------------->
train_opt_callback: iter=   175 sample=176/61337 sched=0.933688 loss=0.221632 dt=00:01:40 eta=02:15:26 |------------------------------------------------------------------------------>
train_opt_callback: iter=   176 sample=177/61337 sched=0.932948 loss=0.359004 dt=00:01:39 eta=02:12:59 |---------------------------------------------------------------------------->
train_opt_callback: iter=   177 sample=178/61337 sched=0.932203 loss=0.338466 dt=00:01:41 eta=02:13:12 |----------------------------------------------------------------------------->
train_opt_callback: iter=   178 sample=179/61337 sched=0.931455 loss=0.055478 dt=00:01:41 eta=02:11:58 |------------------------------------------------------------------------------->
train_opt_callback: iter=   179 sample=180/61337 sched=0.930703 loss=0.030761 dt=00:01:42 eta=02:11:41 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   180 sample=181/61337 sched=0.929948 loss=0.139531 dt=00:01:40 eta=02:07:42 |------------------------------------------------------------------------------->
train_opt_callback: iter=   181 sample=182/61337 sched=0.929188 loss=0.044575 dt=00:01:41 eta=02:07:03 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   182 sample=183/61337 sched=0.928425 loss=0.458600 dt=00:01:41 eta=02:05:27 |--------------------------------------------------------------------------->
train_opt_callback: iter=   183 sample=184/61337 sched=0.927658 loss=0.059777 dt=00:01:42 eta=02:04:14 |------------------------------------------------------------------------------->
train_opt_callback: iter=   184 sample=185/61337 sched=0.926888 loss=0.406161 dt=00:01:42 eta=02:03:26 |---------------------------------------------------------------------------->
train_opt_callback: iter=   185 sample=186/61337 sched=0.926113 loss=0.099174 dt=00:01:41 eta=02:00:02 |------------------------------------------------------------------------------->
train_opt_callback: iter=   186 sample=187/61337 sched=0.925335 loss=0.376780 dt=00:01:41 eta=01:57:52 |---------------------------------------------------------------------------->
train_opt_callback: iter=   187 sample=188/61337 sched=0.924554 loss=0.185592 dt=00:01:40 eta=01:55:55 |------------------------------------------------------------------------------>
train_opt_callback: iter=   188 sample=189/61337 sched=0.923768 loss=0.213829 dt=00:01:41 eta=01:54:36 |------------------------------------------------------------------------------>
train_opt_callback: iter=   189 sample=190/61337 sched=0.922979 loss=0.205349 dt=00:01:40 eta=01:52:41 |------------------------------------------------------------------------------>
train_opt_callback: iter=   190 sample=191/61337 sched=0.922186 loss=0.028160 dt=00:01:40 eta=01:50:22 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   191 sample=192/61337 sched=0.921390 loss=0.128961 dt=00:01:40 eta=01:48:48 |------------------------------------------------------------------------------->
train_opt_callback: iter=   192 sample=193/61337 sched=0.920590 loss=0.409367 dt=00:01:41 eta=01:48:07 |---------------------------------------------------------------------------->
train_opt_callback: iter=   193 sample=194/61337 sched=0.919786 loss=0.069603 dt=00:01:41 eta=01:46:20 |------------------------------------------------------------------------------->
train_opt_callback: iter=   194 sample=195/61337 sched=0.918978 loss=0.091601 dt=00:01:42 eta=01:46:06 |------------------------------------------------------------------------------->
train_opt_callback: iter=   195 sample=196/61337 sched=0.918167 loss=0.248855 dt=00:01:41 eta=01:43:11 |------------------------------------------------------------------------------>
train_opt_callback: iter=   196 sample=197/61337 sched=0.917353 loss=0.147749 dt=00:01:41 eta=01:41:30 |------------------------------------------------------------------------------->
train_opt_callback: iter=   197 sample=198/61337 sched=0.916534 loss=0.029353 dt=00:01:42 eta=01:40:33 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   198 sample=199/61337 sched=0.915712 loss=0.063318 dt=00:01:42 eta=01:39:15 |------------------------------------------------------------------------------->
train_opt_callback: iter=   199 sample=200/61337 sched=0.914887 loss=0.136020 dt=00:01:41 eta=01:36:46 |------------------------------------------------------------------------------->
train_opt_callback: iter=   200 sample=201/61337 sched=0.914058 loss=0.186834 dt=00:01:41 eta=01:34:39 |------------------------------------------------------------------------------>
train_opt_callback: iter=   201 sample=202/61337 sched=0.913225 loss=0.041439 dt=00:01:42 eta=01:34:12 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   202 sample=203/61337 sched=0.912389 loss=0.689315 dt=00:01:41 eta=01:31:47 |------------------------------------------------------------------------->
train_opt_callback: iter=   203 sample=204/61337 sched=0.911549 loss=0.216813 dt=00:01:42 eta=01:30:10 |------------------------------------------------------------------------------>
train_opt_callback: iter=   204 sample=205/61337 sched=0.910705 loss=0.368377 dt=00:01:41 eta=01:28:19 |---------------------------------------------------------------------------->
train_opt_callback: iter=   205 sample=206/61337 sched=0.909858 loss=0.131208 dt=00:01:40 eta=01:25:46 |------------------------------------------------------------------------------->
train_opt_callback: iter=   206 sample=207/61337 sched=0.909007 loss=0.301606 dt=00:01:40 eta=01:23:44 |----------------------------------------------------------------------------->
train_opt_callback: iter=   207 sample=208/61337 sched=0.908153 loss=0.027318 dt=00:01:40 eta=01:22:17 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   208 sample=209/61337 sched=0.907296 loss=0.205679 dt=00:01:40 eta=01:20:04 |------------------------------------------------------------------------------>
train_opt_callback: iter=   209 sample=210/61337 sched=0.906434 loss=0.128378 dt=00:01:40 eta=01:18:27 |------------------------------------------------------------------------------->
train_opt_callback: iter=   210 sample=211/61337 sched=0.905570 loss=0.216259 dt=00:01:40 eta=01:17:16 |------------------------------------------------------------------------------>
train_opt_callback: iter=   211 sample=212/61337 sched=0.904702 loss=0.264581 dt=00:01:41 eta=01:16:07 |----------------------------------------------------------------------------->
train_opt_callback: iter=   212 sample=213/61337 sched=0.903830 loss=0.137216 dt=00:01:40 eta=01:13:38 |------------------------------------------------------------------------------->
train_opt_callback: iter=   213 sample=214/61337 sched=0.902955 loss=0.088273 dt=00:01:40 eta=01:12:03 |------------------------------------------------------------------------------->
train_opt_callback: iter=   214 sample=215/61337 sched=0.902076 loss=0.166352 dt=00:01:41 eta=01:11:19 |------------------------------------------------------------------------------>
train_opt_callback: iter=   215 sample=216/61337 sched=0.901194 loss=0.135589 dt=00:01:41 eta=01:09:06 |------------------------------------------------------------------------------->
train_opt_callback: iter=   216 sample=217/61337 sched=0.900308 loss=0.321438 dt=00:01:41 eta=01:07:42 |----------------------------------------------------------------------------->
train_opt_callback: iter=   217 sample=218/61337 sched=0.899419 loss=0.329098 dt=00:01:42 eta=01:06:28 |----------------------------------------------------------------------------->
train_opt_callback: iter=   218 sample=219/61337 sched=0.898526 loss=0.107585 dt=00:01:41 eta=01:04:33 |------------------------------------------------------------------------------->
train_opt_callback: iter=   219 sample=220/61337 sched=0.897630 loss=0.086555 dt=00:01:40 eta=01:01:44 |------------------------------------------------------------------------------->
train_opt_callback: iter=   220 sample=221/61337 sched=0.896731 loss=0.311950 dt=00:01:40 eta=01:00:18 |----------------------------------------------------------------------------->
train_opt_callback: iter=   221 sample=222/61337 sched=0.895828 loss=0.257417 dt=00:01:40 eta=00:58:40 |----------------------------------------------------------------------------->
train_opt_callback: iter=   222 sample=223/61337 sched=0.894922 loss=0.092787 dt=00:01:40 eta=00:57:12 |------------------------------------------------------------------------------->
train_opt_callback: iter=   223 sample=224/61337 sched=0.894012 loss=0.321866 dt=00:01:42 eta=00:56:18 |----------------------------------------------------------------------------->
train_opt_callback: iter=   224 sample=225/61337 sched=0.893099 loss=0.252951 dt=00:01:42 eta=00:54:25 |----------------------------------------------------------------------------->
train_opt_callback: iter=   225 sample=226/61337 sched=0.892183 loss=0.046960 dt=00:01:42 eta=00:52:45 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   226 sample=227/61337 sched=0.891263 loss=0.049461 dt=00:01:41 eta=00:50:50 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   227 sample=228/61337 sched=0.890340 loss=0.148587 dt=00:01:41 eta=00:49:03 |------------------------------------------------------------------------------->
train_opt_callback: iter=   228 sample=229/61337 sched=0.889413 loss=0.234776 dt=00:01:42 eta=00:47:46 |------------------------------------------------------------------------------>
train_opt_callback: iter=   229 sample=230/61337 sched=0.888483 loss=0.312039 dt=00:01:41 eta=00:45:53 |----------------------------------------------------------------------------->
train_opt_callback: iter=   230 sample=231/61337 sched=0.887550 loss=0.072013 dt=00:01:41 eta=00:43:52 |------------------------------------------------------------------------------->
train_opt_callback: iter=   231 sample=232/61337 sched=0.886613 loss=0.149980 dt=00:01:41 eta=00:42:15 |------------------------------------------------------------------------------->
train_opt_callback: iter=   232 sample=233/61337 sched=0.885674 loss=0.175972 dt=00:01:40 eta=00:40:16 |------------------------------------------------------------------------------>
train_opt_callback: iter=   233 sample=234/61337 sched=0.884730 loss=0.047642 dt=00:01:41 eta=00:39:00 |-------------------------------------------------------------------------------->
train_opt_callback: iter=   234 sample=235/61337 sched=0.883784 loss=0.070447 dt=00:01:41 eta=00:37:10 |------------------------------------------------------------------------------->
train_opt_callback: iter=   235 sample=236/61337 sched=0.882834 loss=0.167914 dt=00:01:41 eta=00:35:40 |------------------------------------------------------------------------------>
train_opt_callback: iter=   236 sample=237/61337 sched=0.881881 loss=0.055650 dt=00:01:41 eta=00:33:40 |------------------------------------------------------------------------------->
train_opt_callback: iter=   237 sample=238/61337 sched=0.880924 loss=0.195111 dt=00:01:40 eta=00:31:58 |------------------------------------------------------------------------------>
train_opt_callback: iter=   238 sample=239/61337 sched=0.879965 loss=0.238734 dt=00:01:40 eta=00:30:04 |------------------------------------------------------------------------------>
train_opt_callback: iter=   239 sample=240/61337 sched=0.879002 loss=0.297598 dt=00:01:40 eta=00:28:35 |----------------------------------------------------------------------------->
train_opt_callback: iter=   240 sample=241/61337 sched=0.878036 loss=0.068985 dt=00:01:41 eta=00:27:03 |------------------------------------------------------------------------------->
train_opt_callback: iter=   241 sample=242/61337 sched=0.877066 loss=0.211844 dt=00:01:41 eta=00:25:19 |------------------------------------------------------------------------------>
train_opt_callback: iter=   242 sample=243/61337 sched=0.876094 loss=0.174178 dt=00:01:40 eta=00:23:28 |------------------------------------------------------------------------------>
train_opt_callback: iter=   243 sample=244/61337 sched=0.875118 loss=0.097950 dt=00:01:40 eta=00:21:49 |------------------------------------------------------------------------------->
train_opt_callback: iter=   244 sample=245/61337 sched=0.874139 loss=0.086203 dt=00:01:40 eta=00:20:06 |------------------------------------------------------------------------------->
train_opt_callback: iter=   245 sample=246/61337 sched=0.873157 loss=0.149381 dt=00:01:40 eta=00:18:27 |------------------------------------------------------------------------------->
train_opt_callback: iter=   246 sample=247/61337 sched=0.872171 loss=0.107111 dt=00:01:41 eta=00:16:51 |------------------------------------------------------------------------------->
train_opt_callback: iter=   247 sample=248/61337 sched=0.871183 loss=0.168964 dt=00:01:41 eta=00:15:15 |------------------------------------------------------------------------------>
train_opt_callback: iter=   248 sample=249/61337 sched=0.870191 loss=0.110861 dt=00:01:40 eta=00:13:24 |------------------------------------------------------------------------------->
train_opt_callback: iter=   249 sample=250/61337 sched=0.869196 loss=0.058816 dt=00:01:40 eta=00:11:46 |------------------------------------------------------------------------------->
train_opt_callback: iter=   250 sample=251/61337 sched=0.868198 loss=0.456755 dt=00:01:40 eta=00:10:05 |--------------------------------------------------------------------------->
train_opt_callback: iter=   251 sample=252/61337 sched=0.867197 loss=0.057576 dt=00:01:41 eta=00:08:25 |------------------------------------------------------------------------------->
train_opt_callback: iter=   252 sample=253/61337 sched=0.866192 loss=0.074070 dt=00:01:41 eta=00:06:45 |------------------------------------------------------------------------------->
train_opt_callback: iter=   253 sample=254/61337 sched=0.865185 loss=0.159082 dt=00:01:41 eta=00:05:04 |------------------------------------------------------------------------------>
train_opt_callback: iter=   254 sample=255/61337 sched=0.864174 loss=0.087604 dt=00:01:41 eta=00:03:22 |------------------------------------------------------------------------------->
train_opt_callback: iter=   255 sample=256/61337 sched=0.863161 loss=0.090159 dt=00:01:40 eta=00:01:40 |------------------------------------------------------------------------------->
train_opt_callback: iter=   256 sample=257/61337 sched=0.862144 loss=0.023769 dt=00:01:41 eta=0.0ms |-------------------------------------------------------------------------------->
main: total training time: 07:13:47
save_checkpoint_lora_file: saving to checkpoint-256.gguf
save_checkpoint_lora_file: saving to checkpoint-LATEST.gguf
save_as_llama_lora: saving to llama2-7b-chat-lora.gguf
save_as_llama_lora: saving to llama2-7b-chat-lora.gguf
